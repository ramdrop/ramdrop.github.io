<!doctype html><html lang="en"><head>  <meta charset="utf-8">  <meta name="keywords" content="">  <meta name="description" content="">  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover"><link rel="canonical" href="https://www.csc.liv.ac.uk/~ramdrop/edgevl.html"><meta name="robots" content="index, follow">  <link rel="shortcut icon" type="image/png" href="favicon.png">  <link rel="stylesheet" type="text/css" href="./css/bootstrap.css?4878"><link rel="stylesheet" type="text/css" href="style.css?1787"><link rel="stylesheet" type="text/css" href="./css/animate.css?2038"><link href='https://fonts.googleapis.com/css?family=Sriracha&display=swap&subset=latin,latin-ext' rel='stylesheet' type='text/css'><link href='https://fonts.googleapis.com/css?family=IBM+Plex+Sans+Arabic:200,300,400,500,600,40&display=swap&subset=latin,latin-ext' rel='stylesheet' type='text/css'>  <title>EdgeVL</title><style>.sup { vertical-align: super;  font-size: smaller;}.sub { vertical-align: sub;  font-size: smaller;}</style><body><!-- Preloader --><div id="page-loading-blocs-notifaction" class="page-preloader"></div><!-- Preloader END -->  <script type="module">  import { initCursor } from "https://unpkg.com/ipad-cursor@latest";  initCursor();  </script></body>  <!-- Analytics --><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-P2S7NVPRS9"></script><script>  window.dataLayer = window.dataLayer || [];  function gtag(){dataLayer.push(arguments);}  gtag('js', new Date());  gtag('config', 'G-P2S7NVPRS9');</script><!-- Analytics END -->  </head><body><!-- Preloader --><div id="page-loading-blocs-notifaction" class="page-preloader"></div><!-- Preloader END --><!-- Main container --><div class="page-container">  <!-- bloc-0 --><div class="bloc sticky-nav none bgc-4488 b-divider-dashed d-bloc" id="bloc-0"><div class="container bloc-sm bloc-no-padding-lg"><div class="row align-items-center voffset"><div class="col"><div class="row"><div class="col-12"><nav class="navbar row navbar-expand-md navbar-light hover-open-submenu"><div class="container-fluid"><a class="navbar-brand link-kaiwen-cai-style ps-lg-3 pe-lg-3 pt-lg-3 pb-lg-3 " href="index.html" data-cursor="block">Kaiwen Cai&nbsp;</a><button id="nav-toggle" type="button" class="ui-navbar-toggler navbar-toggler border-0 p-0 ms-auto me-md-0" aria-expanded="false" aria-label="Toggle navigation" data-bs-toggle="collapse" data-bs-target=".navbar-5128"><span class="navbar-toggler-icon"><svg height="32" viewBox="0 0 32 32" width="32"><path class="svg-menu-icon " d="m2 9h28m-28 7h28m-28 7h28"></path></svg></span></button><div class="collapse navbar-collapse navbar-5128 col-lg-8 special-dropdown-nav offset-lg-2"><ul class="site-navigation nav navbar-nav ms-auto"><li class="nav-item"><a href="publications.html" class="nav-link link-style" data-cursor="block">PUBLICATIONS</a></li><li class="nav-item"><a href="tutorials.html" class="nav-link link-style" data-cursor="block">TUTORIALS</a></li></ul></div></div></nav></div></div></div></div></div></div><!-- bloc-0 END --><!-- bloc-7 --><div class="bloc l-bloc" id="bloc-7"><div class="container bloc-lg bloc-sm-lg"><div class="row"><div class="col"><h3 class="text-lg-center mg-md bold-text" data-cursor="text">Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities</h3><p class="text-lg-center mg-clear" data-cursor="text">(The 18th European Conference on Computer Vision ECCV 2024)</p><p class="text-lg-center mg-clear" data-cursor="text">Kaiwen Cai<span class="sup">1</span>, Zhekai Duan<span class="sup">1</span>, Gaowen Liu<span class="sup">2</span>, Charles Fleming<span class="sup">2</span>, Chris Xiaoxuan Lu<span class="sup">3</span></p><p class="text-lg-center" data-cursor="text"><span class="sup">1</span>University of Edinburgh, <span class="sup">2</span>Cisco Research, <span class="sup">3</span>University College London</p><div class=" html-widget-0-bloc-3-style"><div style="text-align: center;"><a data-cursor="block" data-cursor-style="background: rgba(255, 255, 255, 0.3)" href="https://arxiv.org/abs/2403.04908"><img alt="Static Badge" src="img/lazyload-ph.png" data-src="https://img.shields.io/badge/arXiv-2403.04908-red?color=B13130" class="lazyload"></a>&nbsp;<a data-cursor="block" data-cursor-style="background: rgba(255, 255, 255, 0.3)" href="https://github.com/ramdrop/edgevl"><img alt="GitHub" src="img/lazyload-ph.png" data-src="https://img.shields.io/github/stars/ramdrop/edgevl?style=flat&label=Github&color=black" class="lazyload">&nbsp;</a><a data-cursor="block" data-cursor-style="background: rgba(255, 255, 255, 0.3)" href="https://www.youtube.com/watch?v=2E2XEPTFOKE&ab_channel=KaiwenCai"><img alt="Static Badge" src="img/lazyload-ph.png" data-src="https://img.shields.io/badge/YouTube-Video-red" class="lazyload"></a></div></div></div></div></div></div><!-- bloc-7 END --><!-- bloc-8 --><div class="bloc l-bloc" id="bloc-8"><div class="container bloc-lg bloc-sm-lg"><div class="row"><div class="col"><h4 class="text-lg-center mg-sm bold-text" data-cursor="text">Abstract</h4><p class="bold-text mg-clear" data-cursor="text">TL;DR: we enable large language-visual models like CLIP to work with non-RGB images on resource-constrained devices.</p><p class="p-bloc-8-style" data-cursor="text">Recent advancements in Vision-Language (VL) models have sparked interest in their deployment on edge devices, yet challenges in handling diverse visual modalities, manual annotation, and computational constraints remain. We introduce EdgeVL, a novel framework that bridges this gap by seamlessly integrating dual-modality knowledge distillation and quantization-aware contrastive learning. This approach enables the adaptation of large VL models, like CLIP, for efficient use with both RGB and non-RGB images on resource-limited devices without the need for manual annotations. EdgeVL not only transfers visual language alignment capabilities to compact models but also maintains feature quality post-quantization, significantly enhancing open-vocabulary classification performance across various visual modalities. Our work represents the first systematic effort to adapt large VL models for edge deployment, showcasing up to 15.4% accuracy improvements on multiple datasets and up to 93-fold reduction in model size.</p></div></div></div></div><!-- bloc-8 END --><!-- bloc-9 --><div class="bloc l-bloc" id="bloc-9"><div class="container bloc-lg bloc-sm-lg"><div class="row"><div class="col"><h4 class="mg-md text-lg-center bold-text" data-cursor="text">Video (~40s)</h4><div class="ratio ratio-16x9"><iframe class="embed-responsive-item lazyload" src="img/lazyload-ph.png" data-src="https://www.youtube.com/embed/2E2XEPTFOKE?si=JJrCw8h4LcdlCiFN" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></div></div></div></div></div><!-- bloc-9 END --><!-- bloc-10 --><div class="bloc l-bloc" id="bloc-10"><div class="container bloc-lg bloc-sm-lg"><div class="row"><div class="col"><h4 class="mg-md text-lg-center bold-text" data-cursor="text">Methods</h4><div class="row"><div class="col"><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/edgevl_openfig.webp"><img src="img/lazyload-ph.png" data-src="img/edgevl_openfig.png" class="img-fluid mx-auto d-block img-openf-style lazyload" alt="openfig" width="980" height="350"></picture><p class="p-45-style" data-cursor="text">The adaptation problem of large visual language model to edge devices across visual modalities. We use a resource-constrained cleaning robot as the edge device for illustration. The robot has a co-located RGB and depth cameras, generating many paired images without scene labels. Using RGB-depth pairs as the inputs and the pre-trained image encoder in CLIP as the teacher, EdgeVL is designed to transfer the knowledge to a small student encoder without labels or human intervention. After this learning process, the student encoder can agnostically process either image modalities for open-vocabulary scene classification on the device.<br><br></p></div></div><div class="row voffset"><div class="col"><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/overall_structure.webp"><img src="img/lazyload-ph.png" data-src="img/overall_structure.png" class="img-fluid mx-auto d-block img-pipeli-style lazyload" alt="openfig" width="857" height="355"></picture><p class="p-45-style" data-cursor="text">Overall architecture of our proposed method. In stage-1, we distill the knowledge from the pre-trained visual encoder to the student model. In stage-2, we first fake-quantize the pretrained student model, then use contrastive learning to refine the student model.<br><br></p></div></div><div class="row voffset"><div class="col"><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/result1.webp"><img src="img/lazyload-ph.png" data-src="img/result1.png" class="img-fluid mx-auto d-block lazyload" alt="openfig" width="558" height="379"></picture></div><div class="col align-self-center"><p class="mg-clear mx-auto d-block p-41-style text-lg-start" data-cursor="text"><span class="bold-text">EdgeVL</span>&nbsp;works with popular backbones such as DAT, SwinTransformer, and ViT. Regardless of the backbone chosen, EdgeVL outperforms other methods in terms of open-vocabulary classification accuracy.</p></div></div><div class="row voffset"><div class="col"><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/result2.webp"><img src="img/lazyload-ph.png" data-src="img/result2.png" class="img-fluid img-bloc-10-style float-lg-none lazyload" alt="openfig" width="558" height="135"></picture></div><div class="col align-self-center"><p class="mg-clear mx-auto d-block p-42-style text-lg-start" data-cursor="text"><span class="bold-text">EdgeVL</span> cuts the inference latency in half.<br></p></div></div><div class="row voffset"><div class="col-12"><p class="p-45-style" data-cursor="text">Below are visual examples of the predictions made by CLIP-G, CQD, SKD, and EdgeVL on the EuroSAT, ScanNet, and SUNRGBD datasets.<br></p><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/eurosat.webp"><img src="img/lazyload-ph.png" data-src="img/eurosat.png" class="img-fluid mx-auto d-block lazyload" alt="qua competing" width="1140" height="572"></picture><p class="p-45-style" data-cursor="text"><br></p></div></div><div class="row"><div class="col"><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/scannet.webp"><img src="img/lazyload-ph.png" data-src="img/scannet.png" class="img-fluid mx-auto d-block lazyload" alt="qua competing" width="1140" height="445"></picture><p class="p-45-style" data-cursor="text"><br></p><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/sunrgbd1.webp"><img src="img/lazyload-ph.png" data-src="img/sunrgbd1.png" class="img-fluid mx-auto d-block lazyload" alt="qua competing" width="1140" height="447"></picture><p class="p-45-style" data-cursor="text"><br></p><picture><source type="image/webp" srcset="img/lazyload-ph.png" data-srcset="img/sunrgbd2.webp"><img src="img/lazyload-ph.png" data-src="img/sunrgbd2.png" class="img-fluid mx-auto d-block lazyload " alt="qua competing" width="1140" height="446"></picture><p class="p-45-style float-lg-none btn-resize-mode pt-lg-3 mb-lg-0 mt-lg-3" data-cursor="text">Cite us if you find our work useful:</p><div class="divider-h mt-lg-0 mb-lg-0"></div><p class="p-45-style" data-cursor="text">@inproceedings{cai2024selfadapting,<br> author = {Cai, Kaiwen and Duan, Zhekai and Liu, Gaowen and Fleming, Charles and Lu, Chris Xiaoxuan},<br> booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})}, <br> year = {2024},<br> pages = {},<br> publisher = {},<br> title = {Self-{Adapting} {Large} {Visual}-{Language} {Models} to {Edge} {Devices} across {Visual} {Modalities}},<br>}<br></p></div></div></div></div></div></div><!-- bloc-10 END --><!-- bloc-12 --><div class="bloc l-bloc" id="bloc-12"><div class="container bloc-lg"><div class="row"><div class="col"></div></div></div></div><!-- bloc-12 END --><!-- ScrollToTop Button --><button aria-label="Scroll to top button" class="bloc-button btn btn-d scrollToTop" onclick="scrollToTarget('1',this)"><svg xmlns="http://www.w3.org/2000/svg" width="22" height="22" viewBox="0 0 32 32"><path class="scroll-to-top-btn-icon" d="M30,22.656l-14-13-14,13"/></svg></button><!-- ScrollToTop Button END--></div><!-- Main container END -->  <!-- Additional JS --><script src="./js/jquery.min.js"></script><script src="./js/bootstrap.bundle.js?3651"></script><script src="./js/blocs.js?1497"></script><script src="./js/lazysizes.min.js" defer></script><script src="./js/blocsaddons-markdown.js?2761"></script><script src="./js/marked.min.js?9622"></script><!-- Additional JS END --></body></html>